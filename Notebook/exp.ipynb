{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Google Sheets CSV export URL\n",
    "url = \"https://docs.google.com/spreadsheets/d/1H-b3MA0fEEHH7eP-fut1sYHlRsjc5i8ZadReynrw0m0/export?format=csv&gid=167401221\"\n",
    "\n",
    "# Read into DataFrame\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Preview\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New rows detected: 1\n",
      "     PassengerId  Survived  Pclass            Name   Sex   Age  SibSp  Parch  \\\n",
      "840          841         1       1  Md Kamruzzaman  male  33.0    0.0      0   \n",
      "\n",
      "    Ticket  Fare Cabin Embarked  \n",
      "840  15000  30.0   E29        S  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Step 1: Define URL of the Google Sheet ---\n",
    "url = \"https://docs.google.com/spreadsheets/d/1H-b3MA0fEEHH7eP-fut1sYHlRsjc5i8ZadReynrw0m0/export?format=csv&gid=167401221\"\n",
    "\n",
    "# --- Step 2: Load the full data ---\n",
    "df = pd.read_csv(url)\n",
    "current_len = len(df)\n",
    "\n",
    "# --- Step 3: Define log file path ---\n",
    "log_file = \"data_load.log\"\n",
    "\n",
    "# --- Step 4: Get last loaded row count from the last line ---\n",
    "if os.path.exists(log_file):\n",
    "    with open(log_file, 'r') as log:\n",
    "        lines = log.readlines()\n",
    "        if lines:\n",
    "            last_line = lines[-1]\n",
    "            last_len = int(last_line.strip().split(\" - \")[-1])\n",
    "        else:\n",
    "            last_len = 0\n",
    "else:\n",
    "    last_len = 0  # First time load\n",
    "\n",
    "# --- Step 5: Extract only new rows ---\n",
    "if current_len > last_len:\n",
    "    new_rows = df.iloc[last_len:current_len]\n",
    "    print(f\"✅ New rows detected: {len(new_rows)}\")\n",
    "    print(new_rows)\n",
    "else:\n",
    "    print(\"✅ No new rows found.\")\n",
    "    new_rows = pd.DataFrame()  # Empty DataFrame if nothing new\n",
    "\n",
    "# --- Step 6: Append current row count and timestamp to the log ---\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "with open(log_file, 'a') as log:\n",
    "    log.write(f\"{now} - {current_len}\\n\")\n",
    "\n",
    "# --- Step 7: Save new rows to a CSV (optional) ---\n",
    "if not new_rows.empty:\n",
    "    new_rows.to_csv(\"new_rows.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No new rows found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Step 1: Define URL of the Google Sheet ---\n",
    "url = \"https://docs.google.com/spreadsheets/d/1H-b3MA0fEEHH7eP-fut1sYHlRsjc5i8ZadReynrw0m0/export?format=csv&gid=167401221\"\n",
    "\n",
    "# --- Step 2: Load the full data ---\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# --- Step 3: Transform column names to lowercase ---\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "current_len = len(df)\n",
    "\n",
    "# --- Step 4: Define log file path ---\n",
    "log_file = \"data_load.log\"\n",
    "\n",
    "# --- Step 5: Get last loaded row count from the last valid line ---\n",
    "last_len = 0\n",
    "if os.path.exists(log_file):\n",
    "    with open(log_file, 'r') as log:\n",
    "        lines = [line.strip() for line in log.readlines() if line.strip()]\n",
    "        if lines:\n",
    "            for line in reversed(lines):\n",
    "                try:\n",
    "                    last_len = int(line.split(\" - \")[-1])\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue  # Skip malformed lines\n",
    "\n",
    "# --- Step 6: Extract only new rows ---\n",
    "if current_len > last_len:\n",
    "    new_rows = df.iloc[last_len:current_len]\n",
    "    print(f\"✅ New rows detected: {len(new_rows)}\")\n",
    "    print(new_rows)\n",
    "else:\n",
    "    print(\"✅ No new rows found.\")\n",
    "    new_rows = pd.DataFrame()\n",
    "\n",
    "# --- Step 7: Append current row count and timestamp to the log ---\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "with open(log_file, 'a') as log:\n",
    "    log.write(f\"{now} - {current_len}\\n\")\n",
    "\n",
    "# --- Step 8: Save new rows to a CSV (optional) ---\n",
    "if not new_rows.empty:\n",
    "    new_rows.to_csv(\"new_rows.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Step 1: Define URL of the Google Sheet ---\n",
    "#url = \"https://docs.google.com/spreadsheets/d/1H-b3MA0fEEHH7eP-fut1sYHlRsjc5i8ZadReynrw0m0/export?format=csv&gid=167401221\"\n",
    "url = \"https://drive.google.com/file/d/1Bm9nP2KJE7XMf6tnbGjD1fUVZOcJFor4/view?usp=drive_link\"\n",
    "# --- Step 2: Load the full data ---\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# --- Step 3: Transform column names to lowercase ---\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "current_len = len(df)\n",
    "\n",
    "# --- Step 4: Define log file path ---\n",
    "log_file = \"data_load.log\"\n",
    "\n",
    "# --- Step 5: Get last loaded row count from the last valid line ---\n",
    "last_len = 0\n",
    "if os.path.exists(log_file):\n",
    "    with open(log_file, 'r') as log:\n",
    "        lines = [line.strip() for line in log.readlines() if line.strip()]\n",
    "        if lines:\n",
    "            for line in reversed(lines):\n",
    "                try:\n",
    "                    last_len = int(line.split(\" - \")[-1])\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "# --- Step 6: Extract only new rows ---\n",
    "if current_len > last_len:\n",
    "    new_rows = df.iloc[last_len:current_len]\n",
    "    print(f\"✅ New rows detected: {len(new_rows)}\")\n",
    "    print(new_rows)\n",
    "else:\n",
    "    print(\"✅ No new rows found.\")\n",
    "    new_rows = pd.DataFrame()\n",
    "\n",
    "# --- Step 7: Append current row count and timestamp to the log ---\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "with open(log_file, 'a') as log:\n",
    "    log.write(f\"{now} - {current_len}\\n\")\n",
    "\n",
    "# --- Step 8: Save new rows to a CSV (optional) ---\n",
    "if not new_rows.empty:\n",
    "    new_rows.to_csv(\"new_rows.csv\", index=False)\n",
    "\n",
    "    # --- Step 9: Insert into SQLite database ---\n",
    "    db_path = \"taxi_data.db\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    new_rows.to_sql(\"titanic_data\", conn, if_exists=\"append\", index=False)\n",
    "    conn.close()\n",
    "    print(f\"✅ Inserted {len(new_rows)} new rows into database: {db_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [<!DOCTYPE html><html><head><title>Google Drive - Virus scan warning</title><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\"/><style nonce=\"7G6Dco_JL2mZ5BQf9A0FIg\">.goog-link-button{position:relative;color:#15c;text-decoration:underline;cursor:pointer}.goog-link-button-disabled{color:#ccc;text-decoration:none;cursor:default}body{color:#222;font:normal 13px/1.4 arial, sans-serif;margin:0}.grecaptcha-badge{visibility:hidden}.uc-main{padding-top:50px;text-align:center}#uc-dl-icon{display:inline-block;margin-top:16px;padding-right:1em;vertical-align:top}#uc-text{display:inline-block;max-width:68ex;text-align:left}.uc-error-caption, .uc-warning-caption{color:#222;font-size:16px}#uc-download-link{text-decoration:none}.uc-name-size a{color:#15c;text-decoration:none}.uc-name-size a:visited{color:#61c;text-decoration:none}.uc-name-size a:active{color:#d14836;text-decoration:none}.uc-footer{color:#777;font-size:11px;padding-bottom:5ex;padding-top:5ex;text-align:center}.uc-footer a{color:#15c}.uc-footer a:visited{color:#61c}.uc-footer a:active{color:#d14836}.uc-footer-divider{color:#ccc;width:100%}.goog-inline-block{position:relative;display:-moz-inline-box;display:inline-block}* html .goog-inline-block{display:inline}*:first-child+html .goog-inline-block{display:inline}sentinel{}</style><link rel=\"icon\" href=\"//ssl.gstatic.com/docs/doclist/images/drive_2022q3_32dp.png\"/></head><body><div class=\"uc-main\"><div id=\"uc-dl-icon\" class=\"image-container\"><div class=\"drive-sprite-aux-download-file\"></div></div><div id=\"uc-text\"><p class=\"uc-warning-caption\">Google Drive can't scan this file for viruses.</p><p class=\"uc-warning-subcaption\"><span class=\"uc-name-size\"><a href=\"/open?id=1Bm9nP2KJE7XMf6tnbGjD1fUVZOcJFor4\">yellow_tripdata_2020-01.csv</a> (566M)</span> is too large for Google to scan for viruses. Would you still like to download this file?</p><form id=\"download-form\" action=\"https://drive.usercontent.google.com/download\" method=\"get\"><input type=\"submit\" id=\"uc-download-link\" class=\"goog-inline-block jfk-button jfk-button-action\" value=\"Download anyway\"/><input type=\"hidden\" name=\"id\" value=\"1Bm9nP2KJE7XMf6tnbGjD1fUVZOcJFor4\"><input type=\"hidden\" name=\"confirm\" value=\"t\"><input type=\"hidden\" name=\"uuid\" value=\"04a1441f-d1a8-4d1c-a149-80b04ff0f765\"></form></div></div><div class=\"uc-footer\"><hr class=\"uc-footer-divider\"></div></body></html>]\n",
      "Index: []\n",
      "✅ No new rows found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Convert Google Drive view link to direct download\n",
    "file_id = \"1Bm9nP2KJE7XMf6tnbGjD1fUVZOcJFor4\"\n",
    "csv_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Step 2: Load the CSV correctly\n",
    "df = pd.read_csv(csv_url)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# --- Step 3: Transform column names to lowercase ---\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "current_len = len(df)\n",
    "\n",
    "# --- Step 4: Define log file path ---\n",
    "log_file = \"data_load.log\"\n",
    "\n",
    "# --- Step 5: Get last loaded row count from the last valid line ---\n",
    "last_len = 0\n",
    "if os.path.exists(log_file):\n",
    "    with open(log_file, 'r') as log:\n",
    "        lines = [line.strip() for line in log.readlines() if line.strip()]\n",
    "        if lines:\n",
    "            for line in reversed(lines):\n",
    "                try:\n",
    "                    last_len = int(line.split(\" - \")[-1])\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "# --- Step 6: Extract only new rows ---\n",
    "if current_len > last_len:\n",
    "    new_rows = df.iloc[last_len:current_len]\n",
    "    print(f\"✅ New rows detected: {len(new_rows)}\")\n",
    "    print(new_rows)\n",
    "else:\n",
    "    print(\"✅ No new rows found.\")\n",
    "    new_rows = pd.DataFrame()\n",
    "\n",
    "# --- Step 7: Append current row count and timestamp to the log ---\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "with open(log_file, 'a') as log:\n",
    "    log.write(f\"{now} - {current_len}\\n\")\n",
    "\n",
    "# --- Step 8: Save new rows to a CSV (optional) ---\n",
    "if not new_rows.empty:\n",
    "    new_rows.to_csv(\"new_rows.csv\", index=False)\n",
    "\n",
    "    # --- Step 9: Insert into PostgreSQL ---\n",
    "    postgresql_host = \"localhost\"\n",
    "    postgresql_user = \"postgres\"\n",
    "    postgresql_password = \"123456\"\n",
    "    postgresql_db = \"titanic\"\n",
    "    postgresql_port = \"5433\"\n",
    "\n",
    "    table_name = \"taxi_data\"\n",
    "\n",
    "    # Create connection engine\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{postgresql_user}:{postgresql_password}@{postgresql_host}:{postgresql_port}/{postgresql_db}\"\n",
    "    )\n",
    "\n",
    "    # Insert into table\n",
    "    new_rows.to_sql(table_name, engine, if_exists=\"append\", index=False)\n",
    "    print(f\"✅ Inserted {len(new_rows)} new rows into PostgreSQL table: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'titanic_data' created and data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "url = \"https://docs.google.com/spreadsheets/d/1H-b3MA0fEEHH7eP-fut1sYHlRsjc5i8ZadReynrw0m0/export?format=csv&gid=167401221\"\n",
    "df = pd.read_csv(url)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "postgresql_host = \"localhost\"\n",
    "postgresql_user = \"postgres\"\n",
    "postgresql_password = \"123456\"\n",
    "postgresql_db = \"titanic\"\n",
    "postgresql_port = \"5433\"\n",
    "table_name = \"titanic_data\"\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{postgresql_user}:{postgresql_password}@{postgresql_host}:{postgresql_port}/{postgresql_db}\"\n",
    ")\n",
    "\n",
    "# Replace table if it exists, this will create it freshly\n",
    "df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "print(f\"Table '{table_name}' created and data inserted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_183659/612083757.py:3: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"/home/kamruzaman/Learning/titanic_mlops_projects/Dataset/yellow_tripdata_2020-01.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/kamruzaman/Learning/titanic_mlops_projects/Dataset/yellow_tripdata_2020-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6405008"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6405008 entries, 0 to 6405007\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   VendorID               float64\n",
      " 1   tpep_pickup_datetime   object \n",
      " 2   tpep_dropoff_datetime  object \n",
      " 3   passenger_count        float64\n",
      " 4   trip_distance          float64\n",
      " 5   RatecodeID             float64\n",
      " 6   store_and_fwd_flag     object \n",
      " 7   PULocationID           int64  \n",
      " 8   DOLocationID           int64  \n",
      " 9   payment_type           float64\n",
      " 10  fare_amount            float64\n",
      " 11  extra                  float64\n",
      " 12  mta_tax                float64\n",
      " 13  tip_amount             float64\n",
      " 14  tolls_amount           float64\n",
      " 15  improvement_surcharge  float64\n",
      " 16  total_amount           float64\n",
      " 17  congestion_surcharge   float64\n",
      "dtypes: float64(13), int64(2), object(3)\n",
      "memory usage: 2.0 GB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "half = len(df) // 2  # number of rows to keep\n",
    "df_top_half = df.iloc[:half]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3202504"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_top_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3202504 entries, 0 to 3202503\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   VendorID               float64\n",
      " 1   tpep_pickup_datetime   object \n",
      " 2   tpep_dropoff_datetime  object \n",
      " 3   passenger_count        float64\n",
      " 4   trip_distance          float64\n",
      " 5   RatecodeID             float64\n",
      " 6   store_and_fwd_flag     object \n",
      " 7   PULocationID           int64  \n",
      " 8   DOLocationID           int64  \n",
      " 9   payment_type           float64\n",
      " 10  fare_amount            float64\n",
      " 11  extra                  float64\n",
      " 12  mta_tax                float64\n",
      " 13  tip_amount             float64\n",
      " 14  tolls_amount           float64\n",
      " 15  improvement_surcharge  float64\n",
      " 16  total_amount           float64\n",
      " 17  congestion_surcharge   float64\n",
      "dtypes: float64(13), int64(2), object(3)\n",
      "memory usage: 1007.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_top_half.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_half.to_csv(\"/home/kamruzaman/Learning/titanic_mlops_projects/Dataset/yellow_tripdata_2020-01_top_half.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "half = len(df_top_half) // 2  # number of rows to keep\n",
    "df_top_half = df_top_half.iloc[:half]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1601252"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_top_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/kamruzaman/Learning/titanic_mlops_projects/Dataset/yellow_tripdata_2020-01_top_half.csv\")\n",
    "\n",
    "half = len(df) // 2  # number of rows to keep\n",
    "df_top_half = df.iloc[:half]\n",
    "\n",
    "df_top_half.to_csv(\"/home/kamruzaman/Learning/titanic_mlops_projects/Dataset/yellow_tripdata_2020-01_top_half.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800626"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_top_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_198545/3704162198.py:2: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"/home/kamruzaman/Learning/titanic_mlops_projects/Dataset/yellow_tripdata_2020-01.csv\",low_memory=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"/home/kamruzaman/Learning/titanic_mlops_projects/Dataset/yellow_tripdata_2020-01.csv\",low_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6405008"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\n",
    "    \"postgresql+psycopg2://postgres:123456@localhost:5433/taxi\"\n",
    ")\n",
    "\n",
    "df.to_sql(\n",
    "    \"taxi_data\",\n",
    "    con=engine,\n",
    "    if_df.to_sql(\n",
    "    \"taxi_data\",\n",
    "    con=engine,\n",
    "    if_exists=\"append\",  # or 'replace'\n",
    "    index=False,\n",
    "    chunksize=1000,\n",
    "    method=\"multi\"\n",
    ")\n",
    "exists=\"append\",  # or 'replace'\n",
    "    index=False,\n",
    "    chunksize=100000,\n",
    "    method=\"multi\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlops-project-titanic 0.1 requires imbalanced-learn, which is not installed.\n",
      "mlops-project-titanic 0.1 requires redis, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Using cached imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from imbalanced-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from imbalanced-learn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from imbalanced-learn) (1.7.0)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
      "  Using cached sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from imbalanced-learn) (3.6.0)\n",
      "Collecting scikit-learn<2,>=1.3.2 (from imbalanced-learn)\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Using cached imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Using cached sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Installing collected packages: scikit-learn, sklearn-compat, imbalanced-learn\n",
      "\u001b[2K  Attempting uninstall: scikit-learn\n",
      "\u001b[2K    Found existing installation: scikit-learn 1.7.0\n",
      "\u001b[2K    Uninstalling scikit-learn-1.7.0:\n",
      "\u001b[2K      Successfully uninstalled scikit-learn-1.7.0[0m \u001b[32m0/3\u001b[0m [scikit-learn]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [imbalanced-learn][imbalanced-learn]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlops-project-titanic 0.1 requires redis, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed imbalanced-learn-0.13.0 scikit-learn-1.6.1 sklearn-compat-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (/home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/sklearn/utils/deprecation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n",
      "File \u001b[0;32m~/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/imblearn/__init__.py:52\u001b[0m\n\u001b[1;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         combine,\n\u001b[1;32m     54\u001b[0m         ensemble,\n\u001b[1;32m     55\u001b[0m         exceptions,\n\u001b[1;32m     56\u001b[0m         metrics,\n\u001b[1;32m     57\u001b[0m         over_sampling,\n\u001b[1;32m     58\u001b[0m         pipeline,\n\u001b[1;32m     59\u001b[0m         tensorflow,\n\u001b[1;32m     60\u001b[0m         under_sampling,\n\u001b[1;32m     61\u001b[0m         utils,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[0;32m~/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/imblearn/combine/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/imblearn/combine/_smote_enn.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[0;32m~/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/imblearn/base.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m METHODS\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sklearn_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _fit_context, get_tags, validate_data\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n",
      "File \u001b[0;32m~/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/imblearn/utils/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`imblearn.utils` module includes various utilities.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Substitution\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     check_neighbors_object,\n\u001b[1;32m      8\u001b[0m     check_sampling_strategy,\n\u001b[1;32m      9\u001b[0m     check_target_type,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_neighbors_object\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_sampling_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_target_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubstitution\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/imblearn/utils/_validation.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m type_of_target\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _num_samples\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sklearn_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_pandas_df, check_array\n\u001b[1;32m     22\u001b[0m SAMPLING_KIND \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mover-sampling\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munder-sampling\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbypass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m TARGET_KIND \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/imblearn/utils/_sklearn_compat.py:815\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_dataclass_args())\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTags\u001b[39;00m(Tags):\n\u001b[1;32m    813\u001b[0m     sampler_tags: SamplerTags \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_test_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstance_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    816\u001b[0m     _construct_instances,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m    817\u001b[0m )\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator_checks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    819\u001b[0m     check_estimator,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m    820\u001b[0m     parametrize_with_checks,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m    821\u001b[0m )\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m type_of_target  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/sklearn/utils/_test_common/instance_generator.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone, config_context\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalibration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CalibratedClassifierCV\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     HDBSCAN,\n\u001b[1;32m     15\u001b[0m     AffinityPropagation,\n\u001b[1;32m     16\u001b[0m     AgglomerativeClustering,\n\u001b[1;32m     17\u001b[0m     Birch,\n\u001b[1;32m     18\u001b[0m     BisectingKMeans,\n\u001b[1;32m     19\u001b[0m     FeatureAgglomeration,\n\u001b[1;32m     20\u001b[0m     KMeans,\n\u001b[1;32m     21\u001b[0m     MeanShift,\n\u001b[1;32m     22\u001b[0m     MiniBatchKMeans,\n\u001b[1;32m     23\u001b[0m     SpectralBiclustering,\n\u001b[1;32m     24\u001b[0m     SpectralClustering,\n\u001b[1;32m     25\u001b[0m     SpectralCoclustering,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcovariance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphicalLasso, GraphicalLassoCV\n",
      "File \u001b[0;32m~/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/sklearn/cluster/__init__.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_affinity_propagation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AffinityPropagation, affinity_propagation\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_agglomerative\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     AgglomerativeClustering,\n\u001b[1;32m      9\u001b[0m     FeatureAgglomeration,\n\u001b[1;32m     10\u001b[0m     linkage_tree,\n\u001b[1;32m     11\u001b[0m     ward_tree,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpectralBiclustering, SpectralCoclustering\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_birch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Birch\n",
      "File \u001b[0;32m~/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# mypy error: Module 'sklearn.cluster' has no attribute '_hierarchical_fast'\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _hierarchical_fast \u001b[38;5;28;01mas\u001b[39;00m _hierarchical  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_feature_agglomeration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgglomerationTransform\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# For non fully-connected graphs\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fix_connectivity\u001b[39m(X, connectivity, affinity):\n",
      "File \u001b[0;32m~/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/sklearn/cluster/_feature_agglomeration.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransformerMixin\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_Xt_in_inverse_transform\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_is_fitted, validate_data\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Mixin class for feature agglomeration.\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (/home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages/sklearn/utils/deprecation.py)"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scikit-learn 1.6.1\n",
      "Uninstalling scikit-learn-1.6.1:\n",
      "  Successfully uninstalled scikit-learn-1.6.1\n",
      "Found existing installation: imbalanced-learn 0.13.0\n",
      "Uninstalling imbalanced-learn-0.13.0:\n",
      "  Successfully uninstalled imbalanced-learn-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y scikit-learn imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dagshub\n",
      "  Using cached dagshub-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-2.22.1-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting PyYAML>=5 (from dagshub)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting appdirs>=1.4.4 (from dagshub)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting click>=8.0.4 (from dagshub)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpx>=0.23.0 (from dagshub)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting GitPython>=3.1.29 (from dagshub)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting rich>=13.1.0 (from dagshub)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting dacite~=1.6.0 (from dagshub)\n",
      "  Using cached dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tenacity>=8.2.2 (from dagshub)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting gql[requests] (from dagshub)\n",
      "  Using cached gql-3.5.3-py2.py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting dataclasses-json (from dagshub)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pandas in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from dagshub) (2.3.0)\n",
      "Collecting treelib>=1.6.4 (from dagshub)\n",
      "  Using cached treelib-1.7.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pathvalidate>=3.0.0 (from dagshub)\n",
      "  Using cached pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil in /home/kamruzaman/.local/lib/python3.10/site-packages (from dagshub) (2.9.0.post0)\n",
      "Collecting boto3 (from dagshub)\n",
      "  Using cached boto3-1.38.32-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting semver (from dagshub)\n",
      "  Using cached semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting dagshub-annotation-converter>=0.1.5 (from dagshub)\n",
      "  Using cached dagshub_annotation_converter-0.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mlflow-skinny==2.22.1 (from mlflow)\n",
      "  Downloading mlflow_skinny-2.22.1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting Flask<4 (from mlflow)\n",
      "  Downloading flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting Jinja2<4,>=2.11 (from mlflow)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Using cached graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow)\n",
      "  Using cached gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting markdown<4,>=3.3 (from mlflow)\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting matplotlib<4 (from mlflow)\n",
      "  Using cached matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<3 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from mlflow) (1.26.4)\n",
      "Collecting pandas (from dagshub)\n",
      "  Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting pyarrow<20,>=4.0.0 (from mlflow)\n",
      "  Using cached pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: scikit-learn<2 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from mlflow) (1.3.2)\n",
      "Requirement already satisfied: scipy<2 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from mlflow) (1.15.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from mlflow) (2.0.41)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from mlflow-skinny==2.22.1->mlflow) (5.5.2)\n",
      "Collecting cloudpickle<4 (from mlflow-skinny==2.22.1->mlflow)\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.22.1->mlflow)\n",
      "  Downloading databricks_sdk-0.56.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting fastapi<1 (from mlflow-skinny==2.22.1->mlflow)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.22.1->mlflow)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.22.1->mlflow)\n",
      "  Downloading opentelemetry_api-1.34.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.22.1->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.34.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: packaging<25 in /home/kamruzaman/.local/lib/python3.10/site-packages (from mlflow-skinny==2.22.1->mlflow) (24.2)\n",
      "Collecting protobuf<7,>=3.12.0 (from mlflow-skinny==2.22.1->mlflow)\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3,>=1.10.8 (from mlflow-skinny==2.22.1->mlflow)\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from mlflow-skinny==2.22.1->mlflow) (2.32.3)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.22.1->mlflow)\n",
      "  Using cached sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /home/kamruzaman/.local/lib/python3.10/site-packages (from mlflow-skinny==2.22.1->mlflow) (4.13.1)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==2.22.1->mlflow)\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting tomli (from alembic!=1.10.0,<2->mlflow)\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: google-auth~=2.0 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.1->mlflow) (2.40.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.22.1->mlflow)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow)\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting markupsafe>=2.1.1 (from Flask<4->mlflow)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting werkzeug>=3.1.0 (from Flask<4->mlflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython>=3.1.29->dagshub)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.1->mlflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.1->mlflow) (4.9.1)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Using cached graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Using cached graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.1->mlflow)\n",
      "  Downloading zipp-3.22.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<4->mlflow)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib<4->mlflow)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib<4->mlflow)\n",
      "  Downloading fonttools-4.58.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib<4->mlflow)\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib<4->mlflow)\n",
      "  Using cached pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.2.3)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.1->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.55b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from pandas->dagshub) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from pandas->dagshub) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.1->mlflow)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.1->mlflow)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.1->mlflow)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/kamruzaman/.local/lib/python3.10/site-packages (from python-dateutil->dagshub) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.1->mlflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.1->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.1->mlflow) (2025.4.26)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.1->mlflow) (0.6.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
      "Collecting anyio<5,>=3.6.2 (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.1->mlflow)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/kamruzaman/.local/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.1->mlflow) (1.2.2)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.1->mlflow)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11>=0.8 (from uvicorn<1->mlflow-skinny==2.22.1->mlflow)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting lxml (from dagshub-annotation-converter>=0.1.5->dagshub)\n",
      "  Downloading lxml-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.0->dagshub)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/kamruzaman/.local/lib/python3.10/site-packages (from rich>=13.1.0->dagshub) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/kamruzaman/.local/lib/python3.10/site-packages (from rich>=13.1.0->dagshub) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/kamruzaman/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
      "Collecting botocore<1.39.0,>=1.38.32 (from boto3->dagshub)\n",
      "  Using cached botocore-1.38.32-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->dagshub)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->dagshub)\n",
      "  Using cached s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->dagshub)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->dagshub)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting yarl<2.0,>=1.6 (from gql[requests]->dagshub)\n",
      "  Using cached yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Collecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting requests-toolbelt<2,>=1.0.0 (from gql[requests]->dagshub)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting multidict>=4.0 (from yarl<2.0,>=1.6->gql[requests]->dagshub)\n",
      "  Downloading multidict-6.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.1 (from yarl<2.0,>=1.6->gql[requests]->dagshub)\n",
      "  Using cached propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Using cached dagshub-0.5.10-py3-none-any.whl (260 kB)\n",
      "Using cached dacite-1.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading mlflow-2.22.1-py3-none-any.whl (29.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-2.22.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading databricks_sdk-0.56.0-py3-none-any.whl (733 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.7/733.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading flask-3.1.1-py3-none-any.whl (103 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Using cached graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Using cached matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "Downloading opentelemetry_api-1.34.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_sdk-1.34.0-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.55b0-py3-none-any.whl (196 kB)\n",
      "Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Using cached pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Using cached sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached dagshub_annotation_converter-0.1.9-py3-none-any.whl (33 kB)\n",
      "Downloading fonttools-4.58.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Using cached pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached treelib-1.7.1-py3-none-any.whl (19 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading zipp-3.22.0-py3-none-any.whl (9.8 kB)\n",
      "Using cached boto3-1.38.32-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.38.32-py3-none-any.whl (13.6 MB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Using cached gql-3.5.3-py2.py3-none-any.whl (74 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Downloading multidict-6.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Using cached propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading lxml-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Using cached semver-3.0.4-py3-none-any.whl (17 kB)\n",
      "Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: appdirs, zipp, typing-inspection, treelib, tomli, tenacity, sqlparse, sniffio, smmap, semver, PyYAML, pydantic-core, pyarrow, protobuf, propcache, pillow, pathvalidate, mypy-extensions, multidict, marshmallow, markupsafe, markdown, lxml, kiwisolver, jmespath, itsdangerous, h11, gunicorn, graphql-core, fonttools, dacite, cycler, contourpy, cloudpickle, click, blinker, backoff, annotated-types, yarl, werkzeug, uvicorn, typing-inspect, rich, requests-toolbelt, pydantic, pandas, matplotlib, Mako, Jinja2, importlib_metadata, httpcore, graphql-relay, gitdb, docker, botocore, anyio, starlette, s3transfer, opentelemetry-api, httpx, graphene, gql, GitPython, Flask, dataclasses-json, databricks-sdk, dagshub-annotation-converter, alembic, opentelemetry-semantic-conventions, fastapi, boto3, opentelemetry-sdk, dagshub, mlflow-skinny, mlflow\n",
      "\u001b[2K  Attempting uninstall: pandas━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/75\u001b[0m [pydantic]]re]\n",
      "\u001b[2K    Found existing installation: pandas 2.3.0m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/75\u001b[0m [pydantic]\n",
      "\u001b[2K    Uninstalling pandas-2.3.0:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45/75\u001b[0m [pandas]\n",
      "\u001b[2K      Successfully uninstalled pandas-2.3.0[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45/75\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75/75\u001b[0m [mlflow] [mlflow] [mlflow-skinny]ic-conventions]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlops-project-titanic 0.1 requires redis, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Flask-3.1.1 GitPython-3.1.44 Jinja2-3.1.6 Mako-1.3.10 PyYAML-6.0.2 alembic-1.16.1 annotated-types-0.7.0 anyio-4.9.0 appdirs-1.4.4 backoff-2.2.1 blinker-1.9.0 boto3-1.38.32 botocore-1.38.32 click-8.2.1 cloudpickle-3.1.1 contourpy-1.3.2 cycler-0.12.1 dacite-1.6.0 dagshub-0.5.10 dagshub-annotation-converter-0.1.9 databricks-sdk-0.56.0 dataclasses-json-0.6.7 docker-7.1.0 fastapi-0.115.12 fonttools-4.58.2 gitdb-4.0.12 gql-3.5.3 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 importlib_metadata-8.7.0 itsdangerous-2.2.0 jmespath-1.0.1 kiwisolver-1.4.8 lxml-5.4.0 markdown-3.8 markupsafe-3.0.2 marshmallow-3.26.1 matplotlib-3.10.3 mlflow-2.22.1 mlflow-skinny-2.22.1 multidict-6.4.4 mypy-extensions-1.1.0 opentelemetry-api-1.34.0 opentelemetry-sdk-1.34.0 opentelemetry-semantic-conventions-0.55b0 pandas-2.2.3 pathvalidate-3.2.3 pillow-11.2.1 propcache-0.3.1 protobuf-6.31.1 pyarrow-19.0.1 pydantic-2.11.5 pydantic-core-2.33.2 requests-toolbelt-1.0.0 rich-14.0.0 s3transfer-0.13.0 semver-3.0.4 smmap-5.0.2 sniffio-1.3.1 sqlparse-0.5.3 starlette-0.46.2 tenacity-9.1.2 tomli-2.2.1 treelib-1.7.1 typing-inspect-0.9.0 typing-inspection-0.4.1 uvicorn-0.34.3 werkzeug-3.1.3 yarl-1.20.0 zipp-3.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install dagshub mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.3.2\n",
      "  Using cached scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting imbalanced-learn==0.11.0\n",
      "  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting numpy<2.0,>=1.17.3 (from scikit-learn==1.3.2)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from scikit-learn==1.3.2) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from scikit-learn==1.3.2) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/kamruzaman/miniconda3/envs/titanic_mlops_project/lib/python3.10/site-packages (from scikit-learn==1.3.2) (3.6.0)\n",
      "Using cached scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "Downloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy, scikit-learn, imbalanced-learn\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [imbalanced-learn][imbalanced-learn]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlops-project-titanic 0.1 requires redis, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed imbalanced-learn-0.11.0 numpy-1.26.4 scikit-learn-1.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn==1.3.2 imbalanced-learn==0.11.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: NaNs found in X before SMOTE:\n",
      "Familysize    1\n",
      "dtype: int64\n",
      "NaNs in X after final imputation:\n",
      " Series([], dtype: int64)\n",
      "\n",
      "Original X shape: (846, 11)\n",
      "Resampled X shape: (1032, 11)\n",
      "Original y value counts:\n",
      "Survived\n",
      "0    516\n",
      "1    330\n",
      "Name: count, dtype: int64\n",
      "Resampled y value counts:\n",
      "Survived\n",
      "0    516\n",
      "1    516\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_286584/3272639313.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = X[col].fillna(X[col].median()) # Impute numeric NaNs with median\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"Kamruzzamansust/advance-mlops-projects-\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"Kamruzzamansust/advance-mlops-projects-\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository Kamruzzamansust/advance-mlops-projects- initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository Kamruzzamansust/advance-mlops-projects- initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best Random Forest Parameters: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': 30, 'bootstrap': False}\n",
      "\n",
      "--- Model Evaluation ---\n",
      "Best CV Accuracy: 0.81\n",
      "Test Set Accuracy: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/06/08 23:15:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLflow run completed. Check DagsHub for experiment results.\n",
      "🏃 View run placid-bass-535 at: https://dagshub.com/Kamruzzamansust/advance-mlops-projects-.mlflow/#/experiments/1/runs/a128e25b79fb4dc498078b7a5863d7a4\n",
      "🧪 View experiment at: https://dagshub.com/Kamruzzamansust/advance-mlops-projects-.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import re\n",
    "import mlflow # Import mlflow\n",
    "import dagshub # Import dagshub for easy setup\n",
    "\n",
    "# Load data\n",
    "#url = \"https://docs.google.com/sheets/d/1H-b3MA0fEEHH7eP-fut1sYHlRsjc5i8ZadReynrw0m0/export?format=csv&gid=167401221\"\n",
    "titanic = pd.read_csv(url)\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "\n",
    "# 1. Fill missing values for numerical and categorical columns\n",
    "titanic['Age'] = titanic['Age'].fillna(titanic['Age'].median())\n",
    "titanic['Embarked'] = titanic['Embarked'].fillna(titanic['Embarked'].mode()[0])\n",
    "titanic['Fare'] = titanic['Fare'].fillna(titanic['Fare'].median())\n",
    "\n",
    "# 2. Map categorical to numeric for 'Sex' and 'Embarked'\n",
    "titanic['Sex'] = titanic['Sex'].map({'male': 0, 'female': 1})\n",
    "# Ensure 'Sex' is numeric and handle any NaNs that might result from unmapped values\n",
    "titanic['Sex'] = titanic['Sex'].fillna(0).astype(int) # Fill with 0 (male) as a default\n",
    "\n",
    "titanic['Embarked'] = titanic['Embarked'].astype('category').cat.codes\n",
    "\n",
    "\n",
    "# 3. Feature engineering\n",
    "titanic['Familysize'] = titanic['SibSp'] + titanic['Parch'] + 1\n",
    "titanic['Isalone'] = (titanic['Familysize'] == 1).astype(int)\n",
    "\n",
    "# Handle Cabin column for 'HasCabin' feature\n",
    "titanic['HasCabin'] = titanic['Cabin'].notnull().astype(int)\n",
    "\n",
    "\n",
    "# 4. Safe title extraction function\n",
    "def extract_title(name):\n",
    "    if not isinstance(name, str):\n",
    "        return 'Rare'\n",
    "    match = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return 'Rare'\n",
    "\n",
    "titanic['Title'] = titanic['Name'].apply(extract_title)\n",
    "\n",
    "# Mapping titles to numerical values\n",
    "title_mapping = {\n",
    "    'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3,\n",
    "    'Dr': 4, 'Rev': 4, 'Col': 4, 'Mlle': 4, 'Major': 4, 'Mme': 4,\n",
    "    'Countess': 4, 'Lady': 4, 'Sir': 4, 'Capt': 4, 'Don': 4,\n",
    "    'Ms': 4, 'Jonkheer': 4, 'Dona': 4,\n",
    "    'Rare': 4 # Explicitly include 'Rare' in mapping\n",
    "}\n",
    "titanic['Title'] = titanic['Title'].map(title_mapping).fillna(4).astype(int) # Fill any unmapped titles with 4 (Rare)\n",
    "\n",
    "\n",
    "# 5. Additional features\n",
    "titanic['Pclass_Fare'] = titanic['Pclass'] * titanic['Fare']\n",
    "titanic['Age_Fare'] = titanic['Age'] * titanic['Fare']\n",
    "\n",
    "# --- Prepare X and y ---\n",
    "X = titanic[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Familysize',\n",
    "             'Isalone', 'HasCabin', 'Title', 'Pclass_Fare', 'Age_Fare']]\n",
    "y = titanic['Survived']\n",
    "\n",
    "# --- **CRITICAL: Final NaN Check and Imputation before SMOTE** ---\n",
    "nan_in_X = X.isnull().sum()[X.isnull().sum() > 0]\n",
    "if not nan_in_X.empty:\n",
    "    print(f\"WARNING: NaNs found in X before SMOTE:\\n{nan_in_X}\")\n",
    "    for col in nan_in_X.index:\n",
    "        if pd.api.types.is_numeric_dtype(X[col]):\n",
    "            X[col] = X[col].fillna(X[col].median()) # Impute numeric NaNs with median\n",
    "        else:\n",
    "            print(f\"Non-numeric NaN found in {col}, consider more specific handling. Filling with 0 as fallback.\")\n",
    "            X[col] = X[col].fillna(0) # Fallback for non-numeric NaNs\n",
    "    print(\"NaNs in X after final imputation:\\n\", X.isnull().sum()[X.isnull().sum() > 0])\n",
    "else:\n",
    "    print(\"✅ No NaNs found in X before SMOTE.\")\n",
    "\n",
    "# Ensure y (target variable) has no NaNs\n",
    "if y.isnull().any():\n",
    "    print(f\"WARNING: NaNs found in y. Dropping rows with NaN in y.\")\n",
    "    nan_survived_index = y[y.isnull()].index\n",
    "    X = X.drop(nan_survived_index)\n",
    "    y = y.drop(nan_survived_index)\n",
    "    print(f\"X shape after dropping NaN in y: {X.shape}\")\n",
    "    print(f\"y shape after dropping NaN in y: {y.shape}\")\n",
    "\n",
    "\n",
    "# 6. Balance data with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "print(f\"\\nOriginal X shape: {X.shape}\")\n",
    "print(f\"Resampled X shape: {X_resampled.shape}\")\n",
    "print(f\"Original y value counts:\\n{y.value_counts()}\")\n",
    "print(f\"Resampled y value counts:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "\n",
    "# 7. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# --- MLflow Experiment Tracking Setup ---\n",
    "# Initialize DagsHub/MLflow tracking\n",
    "# Replace 'Kamruzzamansust' with your DagsHub username if different\n",
    "# Replace 'advance-mlops-projects-' with your repository name if different\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/Kamruzzamansust/advance-mlops-projects-.mlflow\")\n",
    "dagshub.init(repo_owner=\"Kamruzzamansust\", repo_name=\"advance-mlops-projects-\", mlflow=True)\n",
    "mlflow.set_experiment(\"Titanic Survival Model Base Line\")\n",
    "\n",
    "\n",
    "# --- MLflow Run ---\n",
    "with mlflow.start_run():\n",
    "    # 8. Random Forest hyperparameter tuning\n",
    "    param_distributions = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    random_search = RandomizedSearchCV(rf, param_distributions, n_iter=30, cv=5,\n",
    "                                       scoring='accuracy', random_state=42, n_jobs=-1, verbose=1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    best_rf = random_search.best_estimator_\n",
    "\n",
    "    # Log best parameters from RandomizedSearchCV\n",
    "    mlflow.log_params(random_search.best_params_)\n",
    "    print(f\"Best Random Forest Parameters: {random_search.best_params_}\")\n",
    "\n",
    "\n",
    "    # 9. Evaluation\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"best_cv_accuracy\", random_search.best_score_)\n",
    "\n",
    "    print(\"\\n--- Model Evaluation ---\")\n",
    "    print(f\"Best CV Accuracy: {random_search.best_score_:.2f}\")\n",
    "    print(f\"Test Set Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "    # Log the trained model\n",
    "    mlflow.sklearn.log_model(best_rf, \"random_forest_model\")\n",
    "\n",
    "    print(\"\\nMLflow run completed. Check DagsHub for experiment results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic_mlops_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
